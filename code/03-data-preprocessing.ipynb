{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "**Purpose**  \n",
    "Make the changes determined in notebook 2 and save them to a new file. Add additional features to improve model performance.\n",
    "\n",
    "*High Level Approach*\n",
    " * Combine all text into single feature\n",
    " * Remove placeholders\n",
    " * Engineer post length and title length features\n",
    " * Engineer sentiment analysis features\n",
    " \n",
    "The amount of data gathered, paired with the eda steps taken in the previous notebook and the feature engineering in this notebook, lead me to believe that my models will predict the subreddit well above the baseline accuracy. Thus, I will be able to answer my problem statement in this particular case, as to how much the accuracy increases as larger training datasets are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpimport\n",
    "url = 'https://raw.githubusercontent.com/zach-brown-18/class-toolkit/main/eda/'\n",
    "with httpimport.remote_repo(['nlp'], url):\n",
    "    import nlp\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/zach-brown-18/class-toolkit/main/feature-engineering/'\n",
    "with httpimport.remote_repo(['nlp_features'], url):\n",
    "    import nlp_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_combine(file1, file2):\n",
    "    '''Loads files and combines into single DataFrame.'''\n",
    "    base_path = '../data/raw/'\n",
    "    df1 = pd.read_csv(base_path + file1)\n",
    "    df2 = pd.read_csv(base_path + file2)\n",
    "    return pd.concat([df1, df2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_len_columns(df):\n",
    "    df['title_length'] = df['title'].map(len)\n",
    "    df['selftext_length'] = df['selftext'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    '''Removes nan and unwanted words. Adds length columns. Comines text into a single column. Drops old text columns. Binarizes target column.'''\n",
    "    new_df = df.copy()\n",
    "    new_df['selftext'] = new_df['selftext'].fillna('').copy()\n",
    "    new_df = new_df.apply(lambda x: x.replace('[removed]', '').replace('[deleted]', ''))\n",
    "    add_len_columns(new_df)\n",
    "    new_df['all_text'] = new_df['title'] + ' ' + new_df['selftext']\n",
    "    new_df['all_text'] = new_df['all_text'].apply(lambda x: nlp.expand_contractions(x))\n",
    "    new_df.drop(columns=['selftext', 'title'], inplace=True)\n",
    "    new_df['subreddit'] = new_df['subreddit'].map({'oceans':1, 'diving':0})\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_data(file1, file2):\n",
    "    return clean(load_data_combine(file1, file2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = load_clean_data('oceans.csv', 'diving.csv')\n",
    "medium = load_clean_data('oceans-medium.csv', 'diving-medium.csv')\n",
    "large = load_clean_data('oceans-large.csv', 'diving-large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small: (3000, 4)\n",
      "Medium: (5300, 4)\n",
      "Large: (7491, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f'Small: {small.shape}')\n",
    "print(f'Medium: {medium.shape}')\n",
    "print(f'Large: {large.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Perspective*  \n",
    "Medium dataset has 77% more posts than the baseline.  \n",
    "Large dataset has 150% more posts than the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_features.add_sentiment_columns(small, 'all_text')\n",
    "nlp_features.add_sentiment_columns(medium, 'all_text')\n",
    "nlp_features.add_sentiment_columns(large, 'all_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [small, medium, large]\n",
    "filenames = ['small.csv', 'medium.csv', 'large.csv']\n",
    "for df, filename in zip(dfs, filenames):\n",
    "    df.to_csv(f'../data/clean/{filename}', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
