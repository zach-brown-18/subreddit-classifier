{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Data\n",
    "Subreddits: Oceans | Diving  \n",
    "Using Reddit's pushshift API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**  \n",
    "Here I gather 3 sets of data per subreddit. Each dataset gathered is larger than the previous one, such that I can quantify the effect of increasing the size of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**  \n",
    "The data is unbalanced in favor of the oceans subreddit. This will be mitigated by using a stratified split when training the model. I have decided to group posts by date instead of forcing equal numbers of posts so that the model will not predict based on time references that are in exclusively one subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "from datetime import timezone\n",
    "import pandas as pd\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_posts(subreddit, date):\n",
    "    '''Accumulate all posts since given date.'''\n",
    "    posts = []\n",
    "    oldest_time = utc_timestamp\n",
    "\n",
    "    while oldest_time > date:\n",
    "\n",
    "        params = {'subreddit': subreddit, 'size': 100, 'before': oldest_time}\n",
    "        try:\n",
    "            some_posts = requests.get(url, params=params).json()['data']\n",
    "            oldest_time = some_posts[-1]['created_utc']\n",
    "            posts.append(some_posts)\n",
    "        except: \n",
    "            oldest_time = some_posts[-1]['created_utc'] - 1209600 # Increase oldest_time by 2 weeks\n",
    "    \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(gather_posts_results):\n",
    "    '''Combines posts into a single list and creates a DataFrame'''\n",
    "    together = [post for group in gather_posts_results for post in group]\n",
    "    df = pd.DataFrame(together).loc[:, ['subreddit', 'selftext', 'title']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit for file check method here https://linuxize.com/post/python-check-if-file-exists/\n",
    "\n",
    "def save_if_no_file_exists(df, filepath):\n",
    "    if os.path.isfile(filepath):\n",
    "        print(f'Data already gathered at {filepath}')\n",
    "    else:\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f'Data saved to {filepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(filepath):\n",
    "    if os.path.isfile(filepath):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_and_save_data(subreddit, oldest_date, filepath):\n",
    "    '''Gather all posts since data and save to a file if that file does not exist already.'''\n",
    "    if file_exists(filepath):\n",
    "        print(f'Data already gathered at {filepath}')\n",
    "        return None\n",
    "    posts = gather_posts(subreddit, oldest_date)\n",
    "    posts_df = build_df(posts)\n",
    "    save_if_no_file_exists(posts_df, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoint\n",
    "url = 'https://api.pushshift.io/reddit/search/submission/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTC time stamp\n",
    "jan_1_2020 = 1577840470\n",
    "jan_1_2019 = 1546300800\n",
    "jan_1_2018 = 1514764800\n",
    "\n",
    "# Current time stamp\n",
    "dt = datetime.datetime.now()\n",
    "utc_time = dt.replace(tzinfo = timezone.utc) \n",
    "utc_timestamp = int(utc_time.timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: https://www.geeksforgeeks.org/get-utc-timestamp-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ocean subreddit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already gathered at ../data/raw/oceans.csv\n",
      "Data already gathered at ../data/raw/oceans-medium.csv\n",
      "Data already gathered at ../data/raw/oceans-large.csv\n"
     ]
    }
   ],
   "source": [
    "gather_and_save_data('oceans', jan_1_2020, '../data/raw/oceans.csv')\n",
    "gather_and_save_data('oceans', jan_1_2019, '../data/raw/oceans-medium.csv')\n",
    "gather_and_save_data('oceans', jan_1_2018, '../data/raw/oceans-large.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diving subreddit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already gathered at ../data/raw/diving.csv\n",
      "Data already gathered at ../data/raw/diving-medium.csv\n",
      "Data already gathered at ../data/raw/diving-large.csv\n"
     ]
    }
   ],
   "source": [
    "gather_and_save_data('diving', jan_1_2020, '../data/raw/diving.csv')\n",
    "gather_and_save_data('diving', jan_1_2019, '../data/raw/diving-medium.csv')\n",
    "gather_and_save_data('diving', jan_1_2018, '../data/raw/diving-large.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
